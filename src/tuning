# 为分层抽样创建类别
df['age_group_stratify'] = pd.cut(df['age'], bins=[0, 20, 30, 40, 50, 80], labels=['0-20', '21-30', '31-40', '41-50', '51+'])
df['stratify_col'] = df['sex'].astype(str) + '_' + df['age_group_stratify'].astype(str)

# 定义自变量X和因变量y
# 包括所有主观体验特征以及年龄和性别
X_features = [
    'age', 'sex',
    'autonomy_freedom', 'autonomy_interesting', 'autonomy_options',
    'competence_matched', 'competence_capable', 'competence_competent',
    'related_important', 'related_fulfilling', 'related_not_close',
    'enjoyment_fun', 'enjoyment_attention', 'enjoymen_boring', 'enjoyment_enjoyed',
    'extrinsic_avoid', 'extrinsic_forget', 'extrinsic_compelled', 'extrinsic_escape',
    'Hours'
]
X = df[X_features]
y = df['happiness_value']

# --- 1. 数据分割 ---
# 我们只需要训练集来进行交叉验证调参
# 按照80%的比例分出训练集
X_train, _, y_train, _ = train_test_split(
    X, y, train_size=0.80, random_state=42, stratify=df['stratify_col']
)

print(f"Using a training set of size: {len(X_train)} for hyperparameter tuning.")
print("-" * 30)


# --- 1. 随机森林 (Random Forest) - 扩展且高效的调参 ---
print("="*50)
print("Starting REFINED Hyperparameter Tuning for Random Forest...")



# 基于你第一次的结果 {'max_depth': 10, 'n_estimators': 300}
# 我们来设计一个新的、更丰富的网格
param_grid_rf_refined = {
    'n_estimators': [280,320],        # 围绕上次最佳值300
    'max_depth': [8,12],               # 围绕上次最佳值10
    'min_samples_split': [5, 10],      # 新增：节点分裂所需的最小样本数
    'min_samples_leaf': [2, 4]         # 新增：叶节点所需的最小样本数
}

rf = RandomForestRegressor(random_state=42)
grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf_refined, 
                              cv=5, n_jobs=-1, scoring='neg_mean_squared_error', verbose=1)

grid_search_rf.fit(X_train, y_train)

# 打印最终结果
print("\n" + "="*40)
print("RESULTS FOR RANDOM FOREST (TABLE 1)")
print("="*40)
print("Optimal Hyperparameters Found:")
print(grid_search_rf.best_params_)
print(f"Best cross-validation MSE: {-grid_search_rf.best_score_:.4f}")
print("="*40 + "\n")


# --- 2. XGBoost - 扩展且高效的调参 ---
print("="*50)
print("Starting REFINED Hyperparameter Tuning for XGBoost...")
print("This may still take a few minutes...")

# 基于你第一次的结果 {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}
# 我们来设计一个新的、包含6个参数的网格
param_grid_xgb_refined = {
    'n_estimators': [100, 150],              # 围绕上次最佳值100
    'max_depth': [3, 4],                      # 围绕上次最佳值3
    'learning_rate': [0.05, 0.1],             # 围绕上次最佳值0.05
    'subsample': [0.7, 0.8],                  # 围绕上次最佳值0.7
    'colsample_bytree': [0.7, 0.8],           # 新增：列采样比例
    'min_child_weight': [1, 3]                # 新增：控制过拟合的关键参数
}

xgb = XGBRegressor(random_state=42, objective='reg:squarederror')
grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb_refined, 
                               cv=5, n_jobs=-1, scoring='neg_mean_squared_error', verbose=1)

grid_search_xgb.fit(X_train, y_train)

# 打印最终结果
print("\n" + "="*40)
print("RESULTS FOR XGBOOST (TABLE 2)")
print("="*40)
print("Optimal Hyperparameters Found:")
print(grid_search_xgb.best_params_)
print(f"Best cross-validation MSE: {-grid_search_xgb.best_score_:.4f}")
print("="*40)
